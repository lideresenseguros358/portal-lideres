/**
 * DIAGNOSTIC: Vertex AI Chat Test
 * ================================
 * Standalone test to verify Vertex AI connectivity for chat.
 * Uses the EXACT same auth pattern as the working email classifier.
 * 
 * GET  /api/diagnostics/vertex-chat          â€” run all tests
 * POST /api/diagnostics/vertex-chat          â€” test with custom message
 */

export const dynamic = 'force-dynamic';
export const runtime = 'nodejs';

import { NextRequest, NextResponse } from 'next/server';
import { GoogleAuth } from 'google-auth-library';

function json(data: any, status = 200) {
  return NextResponse.json(data, { status });
}

// â”€â”€ Test 1: Env vars present â”€â”€
function testEnvVars() {
  const vars = {
    GOOGLE_APPLICATION_CREDENTIALS_JSON: !!process.env.GOOGLE_APPLICATION_CREDENTIALS_JSON,
    GOOGLE_CLOUD_PROJECT_ID: process.env.GOOGLE_CLOUD_PROJECT_ID || '(not set)',
    GOOGLE_CLOUD_LOCATION: process.env.GOOGLE_CLOUD_LOCATION || '(not set â€” will default to us-central1)',
    VERTEX_MODEL_CHAT: process.env.VERTEX_MODEL_CHAT || '(not set â€” will default to gemini-1.5-flash)',
    VERTEX_MODEL_EMAIL: process.env.VERTEX_MODEL_EMAIL || '(not set)',
    VERTEX_API_KEY: process.env.VERTEX_API_KEY ? 'SET (first 10: ' + process.env.VERTEX_API_KEY.substring(0, 10) + '...)' : '(not set)',
  };

  let credentialsParsed = false;
  let credentialsType = '';
  let credentialsProjectId = '';
  let credentialsClientEmail = '';
  try {
    const creds = JSON.parse(process.env.GOOGLE_APPLICATION_CREDENTIALS_JSON || '{}');
    credentialsParsed = true;
    credentialsType = creds.type || '(no type)';
    credentialsProjectId = creds.project_id || '(no project_id)';
    credentialsClientEmail = creds.client_email || '(no client_email)';
  } catch {
    credentialsParsed = false;
  }

  return {
    test: 'env_vars',
    pass: !!process.env.GOOGLE_APPLICATION_CREDENTIALS_JSON && !!process.env.GOOGLE_CLOUD_PROJECT_ID,
    vars,
    credentials: {
      parseable: credentialsParsed,
      type: credentialsType,
      project_id: credentialsProjectId,
      client_email: credentialsClientEmail,
    },
  };
}

// â”€â”€ Test 2: Auth client creation â”€â”€
async function testAuthClient() {
  try {
    const credentialsJson = process.env.GOOGLE_APPLICATION_CREDENTIALS_JSON;
    if (!credentialsJson) return { test: 'auth_client', pass: false, error: 'No credentials JSON' };

    const credentials = JSON.parse(credentialsJson);
    const auth = new GoogleAuth({
      credentials,
      scopes: ['https://www.googleapis.com/auth/cloud-platform'],
    });

    const client = await auth.getClient();
    const tokenResponse = await client.getAccessToken();
    const hasToken = !!tokenResponse?.token;

    return {
      test: 'auth_client',
      pass: hasToken,
      tokenPrefix: hasToken ? tokenResponse.token!.substring(0, 20) + '...' : null,
    };
  } catch (err: any) {
    return { test: 'auth_client', pass: false, error: err.message, stack: err.stack?.substring(0, 200) };
  }
}

// â”€â”€ Test 3: Simple Vertex AI call (same pattern as working email classifier) â”€â”€
async function testVertexSimple() {
  try {
    const credentialsJson = process.env.GOOGLE_APPLICATION_CREDENTIALS_JSON!;
    const credentials = JSON.parse(credentialsJson);
    const auth = new GoogleAuth({
      credentials,
      scopes: ['https://www.googleapis.com/auth/cloud-platform'],
    });
    const client = await auth.getClient();

    const projectId = process.env.GOOGLE_CLOUD_PROJECT_ID!;
    const location = process.env.GOOGLE_CLOUD_LOCATION || 'us-central1';
    const model = process.env.VERTEX_MODEL_CHAT || 'gemini-1.5-flash';

    // EXACT same pattern as working email classifier â€” no systemInstruction
    const endpoint = `https://${location}-aiplatform.googleapis.com/v1/projects/${projectId}/locations/${location}/publishers/google/models/${model}:generateContent`;

    const requestBody = {
      contents: [
        {
          role: 'user',
          parts: [{ text: 'Responde en una oraciÃ³n corta: Â¿QuÃ© es un seguro de auto?' }],
        },
      ],
      generationConfig: {
        temperature: 0.3,
        topP: 0.8,
        topK: 10,
        maxOutputTokens: 256,
      },
    };

    console.log('[VERTEX-TEST] Calling:', endpoint);
    const start = Date.now();

    const response = await client.request({
      url: endpoint,
      method: 'POST',
      data: requestBody,
    });

    const elapsed = Date.now() - start;
    const data: any = response.data;
    const reply = data?.candidates?.[0]?.content?.parts?.[0]?.text || '(no text in response)';
    const tokensUsed = data?.usageMetadata?.totalTokenCount || 0;

    return {
      test: 'vertex_simple_call',
      pass: true,
      reply,
      tokensUsed,
      elapsedMs: elapsed,
      endpoint,
      model,
    };
  } catch (err: any) {
    return {
      test: 'vertex_simple_call',
      pass: false,
      error: err.message,
      status: err.response?.status,
      statusText: err.response?.statusText,
      responseData: JSON.stringify(err.response?.data)?.substring(0, 500),
      code: err.code,
    };
  }
}

// â”€â”€ Test 4: Vertex AI with systemInstruction (Gemini 1.5+ feature) â”€â”€
async function testVertexWithSystemInstruction() {
  try {
    const credentialsJson = process.env.GOOGLE_APPLICATION_CREDENTIALS_JSON!;
    const credentials = JSON.parse(credentialsJson);
    const auth = new GoogleAuth({
      credentials,
      scopes: ['https://www.googleapis.com/auth/cloud-platform'],
    });
    const client = await auth.getClient();

    const projectId = process.env.GOOGLE_CLOUD_PROJECT_ID!;
    const location = process.env.GOOGLE_CLOUD_LOCATION || 'us-central1';
    const model = process.env.VERTEX_MODEL_CHAT || 'gemini-1.5-flash';

    const endpoint = `https://${location}-aiplatform.googleapis.com/v1/projects/${projectId}/locations/${location}/publishers/google/models/${model}:generateContent`;

    const requestBody = {
      systemInstruction: {
        parts: [{ text: 'Eres Lissa, una asistente virtual amigable de una corredurÃ­a de seguros en PanamÃ¡. Hablas en espaÃ±ol, eres cÃ¡lida y cercana. Usas "tÃº" y emojis moderados.' }],
      },
      contents: [
        {
          role: 'user',
          parts: [{ text: 'Hola, quiero saber sobre seguros de auto' }],
        },
      ],
      generationConfig: {
        temperature: 0.7,
        topP: 0.9,
        topK: 40,
        maxOutputTokens: 512,
      },
    };

    const start = Date.now();
    const response = await client.request({
      url: endpoint,
      method: 'POST',
      data: requestBody,
    });

    const elapsed = Date.now() - start;
    const data: any = response.data;
    const reply = data?.candidates?.[0]?.content?.parts?.[0]?.text || '(no text)';
    const tokensUsed = data?.usageMetadata?.totalTokenCount || 0;

    return {
      test: 'vertex_with_system_instruction',
      pass: true,
      reply,
      tokensUsed,
      elapsedMs: elapsed,
    };
  } catch (err: any) {
    return {
      test: 'vertex_with_system_instruction',
      pass: false,
      error: err.message,
      status: err.response?.status,
      responseData: JSON.stringify(err.response?.data)?.substring(0, 500),
    };
  }
}

// â”€â”€ Test 5: Full Lissa chat simulation â”€â”€
async function testLissaChat(userMessage: string) {
  try {
    const credentialsJson = process.env.GOOGLE_APPLICATION_CREDENTIALS_JSON!;
    const credentials = JSON.parse(credentialsJson);
    const auth = new GoogleAuth({
      credentials,
      scopes: ['https://www.googleapis.com/auth/cloud-platform'],
    });
    const client = await auth.getClient();

    const projectId = process.env.GOOGLE_CLOUD_PROJECT_ID!;
    const location = process.env.GOOGLE_CLOUD_LOCATION || 'us-central1';
    const model = process.env.VERTEX_MODEL_CHAT || 'gemini-1.5-flash';

    const endpoint = `https://${location}-aiplatform.googleapis.com/v1/projects/${projectId}/locations/${location}/publishers/google/models/${model}:generateContent`;

    const systemPrompt = `Eres Lissa, la asistente virtual de LÃ­deres en Seguros, una corredurÃ­a de seguros en PanamÃ¡. Hablas como una persona real: cÃ¡lida, empÃ¡tica, cercana. Usas "tÃº" y un tono conversacional natural, como si fueras una amiga que sabe mucho de seguros.

Tu personalidad:
- Eres genuinamente amable y te importa ayudar
- Usas emojis de forma natural pero sin exagerar (ðŸ’š ðŸ‘‹ ðŸ˜Š ðŸ“‹)
- Haces preguntas de seguimiento para entender mejor
- Muestras interÃ©s real en lo que la persona necesita
- NUNCA suenas como un robot ni repites plantillas

Reglas:
1. Siempre en espaÃ±ol
2. No inventas coberturas ni detalles de pÃ³lizas
3. Para cotizar, diriges a: https://portal.lideresenseguros.com/cotizadores
4. Si no puedes resolver algo, ofreces: contacto@lideresenseguros.com o 223-2373`;

    const requestBody = {
      systemInstruction: {
        parts: [{ text: systemPrompt }],
      },
      contents: [
        {
          role: 'user',
          parts: [{ text: userMessage }],
        },
      ],
      generationConfig: {
        temperature: 0.7,
        topP: 0.9,
        topK: 40,
        maxOutputTokens: 1024,
      },
    };

    const start = Date.now();
    const response = await client.request({
      url: endpoint,
      method: 'POST',
      data: requestBody,
    });

    const elapsed = Date.now() - start;
    const data: any = response.data;
    const reply = data?.candidates?.[0]?.content?.parts?.[0]?.text || '(no text)';
    const tokensUsed = data?.usageMetadata?.totalTokenCount || 0;
    const finishReason = data?.candidates?.[0]?.finishReason || 'unknown';

    return {
      test: 'lissa_chat',
      pass: true,
      userMessage,
      reply,
      tokensUsed,
      elapsedMs: elapsed,
      finishReason,
    };
  } catch (err: any) {
    return {
      test: 'lissa_chat',
      pass: false,
      userMessage,
      error: err.message,
      status: err.response?.status,
      responseData: JSON.stringify(err.response?.data)?.substring(0, 500),
    };
  }
}

// â”€â”€ Test 6: Model discovery â€” try many model names to find which works â”€â”€
async function testModelDiscovery() {
  const modelsToTry = [
    'gemini-1.5-flash',
    'gemini-1.5-flash-001',
    'gemini-1.5-flash-002',
    'gemini-1.5-pro',
    'gemini-1.5-pro-001',
    'gemini-1.5-pro-002',
    'gemini-1.0-pro',
    'gemini-1.0-pro-001',
    'gemini-1.0-pro-002',
    'gemini-pro',
    'gemini-2.0-flash',
    'gemini-2.0-flash-001',
    'gemini-2.0-flash-lite',
    'gemini-2.0-pro',
    'gemini-flash-2.0',
    'gemini-2.5-flash',
    'gemini-2.5-pro',
  ];

  const credentialsJson = process.env.GOOGLE_APPLICATION_CREDENTIALS_JSON!;
  const credentials = JSON.parse(credentialsJson);
  const auth = new GoogleAuth({
    credentials,
    scopes: ['https://www.googleapis.com/auth/cloud-platform'],
  });
  const client = await auth.getClient();

  const projectId = process.env.GOOGLE_CLOUD_PROJECT_ID!;
  const location = process.env.GOOGLE_CLOUD_LOCATION || 'us-central1';

  const modelResults: any[] = [];

  for (const model of modelsToTry) {
    const endpoint = `https://${location}-aiplatform.googleapis.com/v1/projects/${projectId}/locations/${location}/publishers/google/models/${model}:generateContent`;
    try {
      const start = Date.now();
      const response = await client.request({
        url: endpoint,
        method: 'POST',
        data: {
          contents: [{ role: 'user', parts: [{ text: 'Responde solo: OK' }] }],
          generationConfig: { temperature: 0, maxOutputTokens: 10 },
        },
      });
      const elapsed = Date.now() - start;
      const data: any = response.data;
      const reply = data?.candidates?.[0]?.content?.parts?.[0]?.text || '';
      modelResults.push({ model, status: 'âœ… WORKS', reply: reply.trim(), elapsedMs: elapsed });
    } catch (err: any) {
      modelResults.push({ model, status: 'âŒ FAILED', error: err.response?.status || err.message });
    }
  }

  const workingModels = modelResults.filter(r => r.status === 'âœ… WORKS');
  return {
    test: 'model_discovery',
    pass: workingModels.length > 0,
    workingModels: workingModels.map(r => r.model),
    recommendedModel: workingModels[0]?.model || null,
    details: modelResults,
  };
}

// â”€â”€ GET handler â€” run all tests â”€â”€
export async function GET() {
  console.log('[VERTEX-TEST] Running diagnostic tests...');

  const results: any[] = [];

  // Test 1: Env vars
  results.push(testEnvVars());

  // Test 2: Auth client
  results.push(await testAuthClient());

  // Test 3: Model discovery â€” find which models work
  const discovery = await testModelDiscovery();
  results.push(discovery);

  // If we found a working model, run the rest of the tests with it
  const workingModel = discovery.recommendedModel;
  if (workingModel) {
    // Override env temporarily for the remaining tests
    const originalModel = process.env.VERTEX_MODEL_CHAT;
    process.env.VERTEX_MODEL_CHAT = workingModel;

    // Test 4: Simple call
    results.push(await testVertexSimple());

    // Test 5: With systemInstruction
    results.push(await testVertexWithSystemInstruction());

    // Test 6-10: Full Lissa simulation
    const testMessages = [
      'Hola, buenos dÃ­as',
      'QuÃ© significa deducible?',
      'Tengo una duda sobre mi cobertura de salud',
      'CuÃ¡nto cuesta un seguro de auto?',
      'Tengo un problema con mi pago y nadie me ayuda',
    ];

    for (const msg of testMessages) {
      results.push(await testLissaChat(msg));
    }

    // Restore
    process.env.VERTEX_MODEL_CHAT = originalModel;
  } else {
    results.push({ test: 'remaining_tests', pass: false, error: 'No working model found â€” skipping chat tests' });
  }

  const allPass = results.every(r => r.pass);

  return json({
    summary: allPass ? 'âœ… ALL TESTS PASSED' : 'âŒ SOME TESTS FAILED',
    timestamp: new Date().toISOString(),
    totalTests: results.length,
    passed: results.filter(r => r.pass).length,
    failed: results.filter(r => !r.pass).length,
    recommendedModel: workingModel || 'NONE FOUND',
    action: workingModel ? `Update VERTEX_MODEL_CHAT in Vercel to: ${workingModel}` : 'Enable Vertex AI API or check model access in GCP',
    results,
  });
}

// â”€â”€ POST handler â€” test with custom message â”€â”€
export async function POST(request: NextRequest) {
  try {
    const body = await request.json();
    const message = body.message || 'Hola, Â¿quÃ© servicios ofrecen?';
    const result = await testLissaChat(message);
    return json(result);
  } catch (err: any) {
    return json({ error: err.message }, 500);
  }
}
